# SCADA Mobile — План разработки

## Контрольные точки

| Дата | Событие |
|---|---|
| **24 февраля** | Старт (сейчас) |
| **20 марта** | Окончание практики — бек и фронт должны быть полностью готовы |
| **середина–конец апреля** | Внедрение: приложение работает у реальных сотрудников на телефонах |
| **начало июня** | Сдача диплома |

---

## Текущее состояние (что уже есть)

| Часть | Статус | Что сделано |
|---|---|---|
| **Backend** | ✅ Работает | Spring Boot, scan cycle (READ→LOGIC→WRITE→UPDATE), TCP-сокет до PrintSrv, in-memory snapshot, pending-буфер, retry/reconnect (3 состояния), REST API (queryAll, setUnitVars, health), OpenAPI/Swagger, CORS, профили dev/prod, логирование MDC |
| **Frontend** | ⚠️ Заглушка | Vanilla HTML/CSS/JS как дизайн-прототип, PWA-обёртка (manifest, service worker, assetlinks.json) |
| **Android** | ✅ Работает | TWA (Bubblewrap), собранный APK |

**Главное ограничение бека сейчас:** `QueryAll` и `SetUnitVars` привязаны к одному устройству `"Line"` — жёстко в коде. Для реальной системы с несколькими линиями нужна параметризация.

---

## Фаза 1 — 24 февраля → 20 марта (практика)

Цель фазы: бек и фронт полностью работают, всё можно показать руководителю.

---

### 24 февр – 5 марта — Бек: поддержка нескольких устройств + WebSocket

- [ ] Параметризовать `QueryAll` и `SetUnitVars` — `deviceName` как параметр запроса, убрать хардкод `"Line"`
- [ ] Обновить контроллер, `ScadaApplicationService` и snapshot-стор — перейти на `Map<deviceName, DeviceSnapshot>`
- [ ] Обновить pending-буфер: ключ `(deviceName, unitNumber)` вместо просто `unitNumber`
- [ ] Scan cycle обходит все устройства из конфига (`printsrv.devices` список)
- [ ] Добавить WebSocket endpoint (Spring WebSocket / STOMP)
  - После каждого scan cycle бек рассылает всем подключённым клиентам два типа сообщений:
    - `SNAPSHOT` — полный снапшот всех устройств (фронт обновляет UI без перезапроса)
    - `ALERT` — событие ошибки: `{ deviceName, unitNumber, errorCode, errorMessage, active: true/false }` — отправляется когда `Error != null && Error != "0"` или `ErrorMessage` не пустой, и отдельно когда ошибка ушла (`active: false`)
- [ ] Реализовать логику определения ошибок в scan cycle: сравнивать новый снапшот с предыдущим, генерировать `ALERT` только при изменении состояния ошибки (появилась / исчезла)
- [ ] Покрыть юнит-тестами: буфер, стор, маппер, параметризацию, детектор ошибок
- [ ] Обновить OpenAPI/Swagger

**Результат:** `/api/v1/commands/queryAll?device=Line`, `/api/v1/commands/queryAll?device=CamAgregation`; реал-тайм пуш снапшота + событий ошибок по WebSocket.

---

### 6 марта – 13 марта — Фронт: React + Vite + TypeScript

- [ ] Инициализировать `frontend/` на Vite + React 18 + TypeScript strict
- [ ] Настроить ESLint + Prettier
- [ ] API-слой `src/api/`: типы из OpenAPI, fetch-клиент, обработка ошибок Problem+JSON
- [ ] WebSocket-хук `useScadaSocket` — единое соединение с беком:
  - При получении `SNAPSHOT` — обновить стейт всех устройств (без мигания, без перезапросов)
  - При получении `ALERT { active: true }` — добавить «живое» уведомление об ошибке
  - При получении `ALERT { active: false }` — автоматически закрыть соответствующее уведомление
  - Fallback при обрыве WebSocket: React Query polling `/queryAll` до переподключения
- [ ] Компоненты UI:
  - `DeviceTabs` / `DeviceSelector` — переключение между устройствами
  - `UnitCard` — карточка юнита (статус, задание, счётчик, ошибка выделена цветом)
  - `StatusBadge` — индикатор (работает / стоит / ошибка)
  - `CommandPanel` — отправка команды на юнит
- [ ] Адаптивная вёрстка — смартфон первичен, цех = плохое освещение, перчатки
- [ ] Тёмная тема
- [ ] Сохранить PWA-файлы: `manifest.webmanifest`, `service-worker.js`, `assetlinks.json`, `netlify.toml`

**Результат:** работающий React-фронт, отображает все устройства и юниты в реальном времени.

---

### 14 марта – 20 марта — Шлифовка, Android, итоговая демонстрация

- [ ] UX-финализация: обработка ошибок (PrintSrv недоступен — последний снапшот + метка времени + предупреждение), пустые состояния, skeleton-загрузка
- [ ] Компонент `AlertBanner` / `AlertOverlay` — sticky-уведомления об ошибках:
  - Появляется мгновенно при получении `ALERT { active: true }` по WebSocket
  - **Не закрывается** пользователем и не исчезает по таймеру — только когда сервер пришлёт `ALERT { active: false }` для этой же пары `(deviceName, unitNumber)`
  - Если ошибок несколько — стек уведомлений, каждое живёт независимо
  - Визуально выделено: красный цвет, иконка, вибрация на мобильном при появлении
  - На экране блокировки / фоне: Web Push API через Service Worker (опционально, если пользователь дал разрешение)
- [ ] Обновить `twa-manifest.json` под production URL фронтенда
- [ ] Пересобрать APK через Bubblewrap — проверить TWA (нет адресной строки, offline)
- [ ] Приёмочное тестирование полной связки: бек → PrintSrv → фронт → Android

**Результат:** система работает end-to-end, APK готов, практика закрыта.

---

## Фаза 2 — 21 марта → ~20 апреля (путь к внедрению)

Цель фазы: система уходит в production на серверы компании, сотрудники начинают пользоваться.

---

### 21 марта – 31 марта — Observability: Prometheus + Grafana

- [ ] Подключить `micrometer-registry-prometheus` к Spring Boot бэку
- [ ] Экспортировать ключевые метрики:
  - `scada_scan_cycle_duration_seconds` — длительность цикла
  - `scada_printsrv_connection_state` — состояние соединения (штатный / переподключение / восстановление)
  - `scada_pending_buffer_size` — размер буфера команд
  - `scada_snapshot_age_seconds` — возраст снапшота (критично для алертов)
  - `scada_websocket_sessions_active` — количество активных клиентов
- [ ] Написать `Dockerfile` для бека (multi-stage build, JRE 21 slim)
- [ ] Написать `docker-compose.yml`: backend + Prometheus + Grafana
- [ ] Настроить Grafana dashboard: состояние соединения PrintSrv, таймлайн ошибок, активные клиенты

**Результат:** локально поднимается полный стек командой `docker compose up`, метрики видны в Grafana.

---

### 1 апреля – 10 апреля — Kubernetes + CI/CD

- [ ] Написать Kubernetes манифесты (или Helm-чарт):
  - `Deployment` для бека (liveness/readiness пробы на существующие `/health/*` endpoints)
  - `Service` + `Ingress` для фронтенда и бека
  - `ConfigMap` для конфига PrintSrv (ip, port, devices)
  - `Secret` для чувствительных параметров
- [ ] `ServiceMonitor` для Prometheus Operator (если в кластере уже есть)
- [ ] Настроить CI/CD в GitHub Actions:
  - build + тесты бека
  - build фронтенда (Vite)
  - сборка и push Docker-образа бека в registry компании (или GitHub Container Registry)
- [ ] Production env-конфигурация через env-переменные без хардкодов

**Результат:** `kubectl apply -f k8s/` или `helm install` поднимает всю систему. CI прогоняет тесты и собирает образ при каждом push в main.

---

### 11 апреля – 20 апреля — Деплой на серверы компании + приёмка

- [ ] Развернуть систему в инфраструктуре предприятия (внутренняя сеть)
- [ ] Подключить к реальному PrintSrv на production-линиях
- [ ] Передать финальный APK сотрудникам, установить на телефоны
- [ ] Алерты в Grafana/Alertmanager: PrintSrv offline > 60 сек, снапшот устарел > 30 сек
- [ ] Написать инструкцию для сисадминов компании: как обновлять, как смотреть логи и метрики
- [ ] Smoke-тест в production: все устройства отображаются, команды доходят

**Результат:** приложение работает у реальных сотрудников.

---

## Фаза 3 — май → начало июня (диплом)

Цель фазы: письменная часть диплома, код уже не трогается (или минимально).

- Оформить `ARCHITECTURE.md` — архитектурные диаграммы, описание слоёв, принятые решения
- Описать весь жизненный цикл: событие на оборудовании → PrintSrv → бек → WebSocket → фронт → Android
- Написать раздел по observability: что мониторится и почему
- Описать инфраструктуру: Docker, K8s, CI/CD
- Оформить по требованиям университета

---

## Ключевые риски

| Риск | Вероятность | Что делать |
|---|---|---|
| PrintSrv недоступен для тестирования локально | Средняя | Mock PrintSrv / тест-стаб с данными из `Br1_Mark/` |
| URL для TWA меняется при деплое фронтенда | Высокая | Обновить `twa-manifest.json` + `assetlinks.json`, пересобрать APK |
| Хардкод `"Line"` остался где-то кроме контроллера | Средняя | Grep по всей кодовой базе перед рефакторингом |
| Задержка с доступом к production-серверу компании | Средняя | Тестировать на локальной `Br1_Mark/` связке, деплой — последний шаг |
| PrintSrv не допускает несколько одновременных TCP-соединений | Низкая | Проверить при тестировании; при необходимости — один scan cycle для всех устройств в одном соединении |
